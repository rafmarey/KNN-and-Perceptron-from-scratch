# -*- coding: utf-8 -*-
"""Dataset_Indicadores_V2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e9VrXt3PubTF0UvofIfpVA0t7lgjC6en
"""

import pandas as pd
import numpy as np

#load dataset
path = "/content/Indicadores_municipales_sabana_DA.csv"
df = pd.read_csv(path, encoding='latin-1')

#drop empty collumns
df = df.dropna(axis=1, how='all')

#fill all the empty cells of the dataset with the mean of its column
df.fillna(df.mean(), inplace=True)

#if there is still empty cells, because of the categorical values, it will fill them with the mode of the collumn
for column in df.columns:
    if df[column].isnull().sum() > 0:
        moda = df[column].mode()[0]
        df[column].fillna(moda, inplace=True)

#count the number of empty cells
empty_cell = df.isnull().sum().sum()
print(f"Empty cells: {empty_cell}")

#delete the unnecessary columns
columns_to_delete = ['ent', 'nom_ent', 'mun', 'clave_mun','nom_mun']
df = df.drop(columns=columns_to_delete)

#change the categorical values to numerical
mapping = {'Muy bajo': 1, 'Bajo': 2, 'Medio': 3, 'Alto': 4, 'Muy alto': 5}
columns_to_convert = ['gdo_rezsoc00', 'gdo_rezsoc05', 'gdo_rezsoc10']  #columns where the changes will be applied

for columna in columns_to_convert:
    df[columna] = df[columna].replace(mapping)

#see the changes
cathegorical_column = ['gdo_rezsoc00', 'gdo_rezsoc05', 'gdo_rezsoc10']

print(df[cathegorical_column].tail())

#move the label column to the end of the dataframe
labels_y = 'gdo_rezsoc10'
df = df[[col for col in df if col != labels_y]
        + [labels_y]]

#print the dataframe
df

"""#KNN without libraries"""

#this is to set a specific seed for reproducibility
np.random.seed(0)

#shuffle the data
df = df.sample(frac=1)

#divide the dataset in 80% for training and 20% for testing
train_size = int(0.8 * len(df))
train_set = df[:train_size]
test_set = df[train_size:]

#declare the labels and features
X_train = train_set.iloc[:, :-1].values
y_train = train_set.iloc[:, -1].values
X_test = test_set.iloc[:, :-1].values
y_test = test_set.iloc[:, -1].values

#obtain mean and standard deviation
mean = np.mean(X_train, axis=0)
std = np.std(X_train, axis=0)

#normalize the data
X_train = (X_train - mean) / std
X_test = (X_test - mean) / std

#define the knn algorithm
def knn(X_train, y_train, X_test, k):
    y_pred = []
    for test_point in X_test:
        distances = np.sqrt(np.sum((X_train - test_point)**2, axis=1))
        k_indices = np.argsort(distances)[:k]
        k_nearest_labels = y_train[k_indices]
        pred = np.bincount(k_nearest_labels).argmax()
        y_pred.append(pred)
    return np.array(y_pred)

#stablish the number of k neighbors
k = 25

#prediction to call the knn function
y_pred = knn(X_train, y_train, X_test, k)

#check the accuracy
accuracy = np.sum(y_pred == y_test) / len(y_test)
print(f"Accuracy: {accuracy * 100:.2f}%")

#select 5 random samples from the testing set
random_samples = np.random.randint(0, len(y_test), 5)

#get the predictions and actual labels
predictions = y_pred[random_samples]
real_labels = y_test[random_samples]

#print results
for i in range(5):
    print(f"Sample {i+1}:")
    print(f"    Prediction: {predictions[i]}")
    print(f"    Actual label: {real_labels[i]}")

"""#KNN with libraries"""

!pip install numpy scikit-learn

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier

#define features and labels
X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values

#divide the dataset in 80% for training and 20% for testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)

#standardize the features of the datasets to get a mean of 0 and a variance of 1
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

k = 25  #stablish the number of k neighbors
model = KNeighborsClassifier(n_neighbors=k)
model.fit(X_train, y_train)

#make predictions
y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

#select 5 random samples from the testing set
random_samples = np.random.randint(0, len(y_test), 5)

#get the predictions and actual labels
predictions = y_pred[random_samples]
real_labels = y_test[random_samples]

#print results
for i in range(5):
    print(f"Sample {i+1}:")
    print(f"    Prediction: {predictions[i]}")
    print(f"    Actual label: {real_labels[i]}")

"""#Perceptron without libraries"""

#this is to set a specific seed for reproducibility
np.random.seed(1)

#shuffle the data
df = df.sample(frac=1)

#divide the dataset: 80% for training and 20% for testing
train_size = int(0.8 * len(df))
test_set = df.iloc[train_size:]
train_set = df.iloc[:train_size]

X_train = train_set.iloc[:, :-1].values
y_train = train_set.iloc[:, -1].values
X_test = test_set.iloc[:, :-1].values
y_test = test_set.iloc[:, -1].values

#convert the labels to binary where the output is 1 if the class is 1 and 0 otherwise
y_binary = np.where(y_train == 1, 1, 0)
y_train_binary = y_binary
y_test_binary = np.where(y_test == 1, 1, 0)

#normalize the training data
mean = np.mean(X_train, axis=0)
std = np.std(X_train, axis=0)
X_train = (X_train - mean) / std

#normalize the testing data
X_test = (X_test - mean) / std

#define the activation function
def activation(x):
    return np.where(x >= 0, 1, 0)

#create the perceptron algorithm
def perceptron(X, y, learning_rate, epochs):
    weights = np.random.randn(X.shape[1]) * 0.01
    bias = 0

    for _ in range(epochs):
        for i in range(X.shape[0]):
            linear_output = np.dot(X[i], weights) + bias
            y_pred = activation(linear_output)
            update = learning_rate * (y[i] - y_pred)
            weights += update * X[i]
            bias += update

    return weights, bias

#define the parameters
learning_rate = 0.1
epochs = 1000

weights, bias = perceptron(X_train, y_train_binary, learning_rate, epochs)

#make predictions
linear_output_test = np.dot(X_test, weights) + bias
y_pred_test = activation(linear_output_test)

#calculate accuracy of the model
accuracy_test = np.sum(y_pred_test == y_test_binary) / len(y_test_binary)
print(f"Accuracy: {accuracy_test * 100:.2f}%")

#select 5 random samples from the testing set
random_samples = np.random.randint(0, len(y_test), 5)

#get the predictions and actual labels
predictions = y_pred_test[random_samples]
real_labels = y_test_binary[random_samples]

#print results
for i in range(5):
    print(f"Sample {i+1}:")
    print(f"   Prediction: {predictions[i]}")
    print(f"   Actual label: {real_labels[i]}")

"""#Perceptron with libraries"""

#import the library
from sklearn.linear_model import Perceptron

#obtain the features and labels from the dataset
X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values

#convert labels to binary: 1 for class "1" and 0 for any other class
y_binary = np.where(y == 1, 1, 0)

#split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)

#normalize the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

#train the model
model = Perceptron(max_iter=1000, eta0=0.1, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

#calculate accuracy of the model
accuracy_test = np.sum(y_pred == y_test) / len(y_test)
print(f"Accuracy: {accuracy_test * 100:.2f}%")

#select 5 random samples from the testing set
random_samples = np.random.randint(0, len(y_test), 5)

#get the predictions and actual labels
predictions = y_pred[random_samples]
real_labels = y_test[random_samples]

#print results
for i in range(5):
    print(f"Sample {i+1}:")
    print(f"   Prediction: {predictions[i]}")
    print(f"   Actual label: {real_labels[i]}")