# KNN-and-Perceptron-from-scratch
I wrote a code for both KNN and perceptron algorithm, using only pandas and numpy.

(Note: it is better if you use the notebook version to observe each output better)

To prepare the dataset for analysis, I first downloaded it from the Datos Abiertos - INEGI page as indicated during our class. I inspected the data and noticed it was a huge dataset. To make sure there were no empty columns I removed them for clarity. For the numeric columns that had missing values, I replaced them with the mean value of that specific column. This method minimizes the disturbance of the data's natural distribution. On the other hand, for those columns containing categorical values, I decided to replace the empty slots with the mode of that column, given that the mode represents the most frequently appearing category. This common approach ensures a representative fill for the missing values. After these initial adjustments, I double-checked for any remaining empty cells. Once I confirmed their count was down to zero, I felt more assured about the data's completeness.
Given the dataset's structure, I decided to remove its first four columns. These primarily served as identifiers for "entities" and "municipalities", and their exclusion simplified the dataset without losing essential information. As for the categorical columns with varying descriptors like "muy bajo" and "bajo", I transformed them into numerical representations, where "muy bajo" converted to 1, "bajo" to 2, and so on. Finally, to make it easier to access during the model training phase, I shifted the "gdo_rezsoc10" label column to the dataset's end, positioning it as the target variable.
 
For the predictive model training, I employed two primary algorithms: KNN (K-Nearest Neighbors) and the Perceptron. Both algorithms were chosen based on their functionality and usefulness in classification problems and their simplicity.
Using the KNN algorithm, I divided the dataset into an 80% training set and a 20% testing set. The features were then normalized, to make sure they had a consistent scale. This is important for KNN since it relies on distances between data points to make predictions. Given the dataset's size, I chose 25 as the number of neighbors and when experimenting with other values I got less accuracy. 
For the Perceptron algorithm, data normalization was also a priority. This is crucial for the Perceptron as it helps ensure faster and more consistent training. With a learning rate of 0.1 and iterating for 1000 epochs, the Perceptron updated its weights based on misclassifications because it is an error-driven algorithm. The weight of each feature was adjusted in the direction that would correct the error in every epoch.
 
After training the models using those algorithms, I proceeded to evaluate their performance using the 20% testing set. This evaluation is crucial, as it allows you to test how well the models might perform on unseen data.
For the KNN algorithm, predictions were generated for the testing set by finding the 'k' nearest neighbors for each data point in the testing set, from the training set. The most frequent class among these neighbors was then assigned as the predicted class for the given test data point. The accuracy was computed by comparing these predictions against the actual labels of the test set.
In the same way, for the Perceptron, predictions on the testing set were obtained by multiplying the test data features with the trained weights and adding the bias. The result was passed through the activation function, which for this binary classification task, produced an output of either 1 or 0. The accuracy was subsequently calculated by comparing the Perceptron's predictions with the actual test labels.
With both algorithms, I randomly selected 5 samples from the testing set and displayed their predicted labels together with the actual labels. This allowed to observe with examples how the models are performing.
 
When comparing the results from the algorithms from scratch with the ones obtained using libraries, it was evident that the ones with libraries have some advantages in terms of optimization and computational efficiency, like the time it takes to finish the training in the case of perceptron. The sklearn library offers very optimized algorithms that provide faster computation times and sometimes even better accuracies, thanks to the extensive research and development behind these libraries.
However, coding the algorithms from scratch was a good experience. It really helped me understand the intuition behind these algorithms, I consider it was a good practice to improve my programming skills. While the accuracy of the models I built from scratch was not bad, the time it takes to train the model without libraries is not suitable if you are trying to do many tests, it takes a while.
 
During this project, I learned several useful things that will help me in the future. First, the process of preparing the dataset for analysis was crucial. Handling missing values, converting categorical data to numerical, and ensuring data consistency were tasks that required careful attention. Inconsistent or improperly processed data could lead to misleading results.
Another challenge was coding the algorithms from scratch without using any advanced libraries other than numpy and pandas. This process required an understanding of the intuition behind the algorithms, as well as to translate that knowledge into code. Debugging and ensuring that these algorithms worked as intended was another issue.
Additionally, the choice of hyperparameters played a significant role in model performance. Finding the right parameters, such as the learning rate in the perceptron or the number of neighbors in the kNN, was essential to achieve optimal results.
In the context of Robotics, these experiences and challenges are very relevant. Robotic systems often have to process large amounts of data, often in real time. Ensuring that this data is accurate and processed correctly is crucial for the functioning of a robot. Whether it's a robot navigating a room, a drone flying in an environment, or a robotic arm picking objects, the principles of data preprocessing and analysis remain the same.
